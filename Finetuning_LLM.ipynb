{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11019798,"sourceType":"datasetVersion","datasetId":6861785}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Install required libraries","metadata":{}},{"cell_type":"code","source":"!pip install unsloth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:08:29.891664Z","iopub.execute_input":"2025-03-14T16:08:29.892004Z","iopub.status.idle":"2025-03-14T16:11:28.970219Z","shell.execute_reply.started":"2025-03-14T16:08:29.891980Z","shell.execute_reply":"2025-03-14T16:11:28.968785Z"}},"outputs":[{"name":"stdout","text":"Collecting unsloth\n  Downloading unsloth-2025.3.13-py3-none-any.whl.metadata (59 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.3/59.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting unsloth_zoo>=2025.3.11 (from unsloth)\n  Downloading unsloth_zoo-2025.3.11-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (2.5.1+cu121)\nCollecting xformers>=0.0.27.post2 (from unsloth)\n  Downloading xformers-0.0.29.post3-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\nCollecting bitsandbytes (from unsloth)\n  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nCollecting triton>=3.0.0 (from unsloth)\n  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from unsloth) (24.2)\nCollecting tyro (from unsloth)\n  Downloading tyro-0.9.17-py3-none-any.whl.metadata (9.5 kB)\nCollecting transformers!=4.47.0,>=4.46.1 (from unsloth)\n  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.3.1)\nRequirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.2.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unsloth) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth) (5.9.5)\nRequirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.45.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth) (1.26.4)\nRequirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (1.2.1)\nCollecting trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9 (from unsloth)\n  Downloading trl-0.15.2-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.14.0)\nRequirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.20.3)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.29.0)\nRequirement already satisfied: hf_transfer in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.1.9)\nRequirement already satisfied: diffusers in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.31.0)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.20.1+cu121)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.1->unsloth) (0.4.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2.32.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.16.0->unsloth) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.11.12)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->unsloth) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (0.21.0)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (13.9.4)\nCollecting cut_cross_entropy (from unsloth_zoo>=2025.3.11->unsloth)\n  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from unsloth_zoo>=2025.3.11->unsloth) (11.0.0)\nCollecting torch>=2.4.0 (from unsloth)\n  Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparselt-cu12==0.6.2 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting nvidia-nccl-cu12==2.21.5 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers->unsloth) (8.5.0)\nINFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\nCollecting torchvision (from unsloth)\n  Downloading torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\nRequirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (0.16)\nCollecting shtab>=1.5.6 (from tyro->unsloth)\n  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (4.4.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.18.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2025.1.31)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (2.19.1)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers->unsloth) (3.21.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->unsloth) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->unsloth) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (0.1.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.17.0)\nDownloading unsloth-2025.3.13-py3-none-any.whl (194 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.9/194.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading trl-0.15.2-py3-none-any.whl (318 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading unsloth_zoo-2025.3.11-py3-none-any.whl (123 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading xformers-0.0.29.post3-cp310-cp310-manylinux_2_28_x86_64.whl (43.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl (766.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading tyro-0.9.17-py3-none-any.whl (123 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.7/123.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\nDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\nInstalling collected packages: triton, nvidia-cusparselt-cu12, shtab, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tyro, nvidia-cusolver-cu12, torch, cut_cross_entropy, transformers, trl, xformers, unsloth_zoo, torchvision, bitsandbytes, unsloth\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.23.4\n    Uninstalling nvidia-nccl-cu12-2.23.4:\n      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.7.77\n    Uninstalling nvidia-curand-cu12-10.3.7.77:\n      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n  Attempting uninstall: torch\n    Found existing installation: torch 2.5.1+cu121\n    Uninstalling torch-2.5.1+cu121:\n      Successfully uninstalled torch-2.5.1+cu121\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.47.0\n    Uninstalling transformers-4.47.0:\n      Successfully uninstalled transformers-4.47.0\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.20.1+cu121\n    Uninstalling torchvision-0.20.1+cu121:\n      Successfully uninstalled torchvision-0.20.1+cu121\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\npylibcugraph-cu12 24.10.0 requires pylibraft-cu12==24.10.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.10.0 requires rmm-cu12==24.10.*, but you have rmm-cu12 25.2.0 which is incompatible.\ntorchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bitsandbytes-0.45.3 cut_cross_entropy-25.1.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 shtab-1.7.1 torch-2.6.0 torchvision-0.21.0 transformers-4.49.0 triton-3.2.0 trl-0.15.2 tyro-0.9.17 unsloth-2025.3.13 unsloth_zoo-2025.3.11 xformers-0.0.29.post3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":" !pip install sacrebleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:11:28.971843Z","iopub.execute_input":"2025-03-14T16:11:28.972185Z","iopub.status.idle":"2025-03-14T16:11:36.658094Z","shell.execute_reply.started":"2025-03-14T16:11:28.972130Z","shell.execute_reply":"2025-03-14T16:11:36.656968Z"}},"outputs":[{"name":"stdout","text":"Collecting sacrebleu\n  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2024.11.6)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (5.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->sacrebleu) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-3.1.1 sacrebleu-2.5.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install  datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:11:36.660052Z","iopub.execute_input":"2025-03-14T16:11:36.660470Z","iopub.status.idle":"2025-03-14T16:11:40.722866Z","shell.execute_reply.started":"2025-03-14T16:11:36.660427Z","shell.execute_reply":"2025-03-14T16:11:40.722000Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install  unbabel-comet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:11:40.724132Z","iopub.execute_input":"2025-03-14T16:11:40.724407Z","iopub.status.idle":"2025-03-14T16:11:59.008961Z","shell.execute_reply.started":"2025-03-14T16:11:40.724382Z","shell.execute_reply":"2025-03-14T16:11:59.008141Z"}},"outputs":[{"name":"stdout","text":"Collecting unbabel-comet\n  Downloading unbabel_comet-2.2.4-py3-none-any.whl.metadata (19 kB)\nCollecting entmax<2.0,>=1.1 (from unbabel-comet)\n  Downloading entmax-1.3-py3-none-any.whl.metadata (348 bytes)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (0.29.0)\nCollecting jsonargparse==3.13.1 (from unbabel-comet)\n  Downloading jsonargparse-3.13.1-py3-none-any.whl.metadata (55 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (1.26.4)\nRequirement already satisfied: pandas>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (2.2.3)\nCollecting protobuf<5.0.0,>=4.24.4 (from unbabel-comet)\n  Downloading protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\nRequirement already satisfied: pytorch-lightning<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (2.5.0.post0)\nRequirement already satisfied: sacrebleu<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (2.5.1)\nRequirement already satisfied: scipy<2.0.0,>=1.5.4 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (1.13.1)\nRequirement already satisfied: sentencepiece<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (0.2.0)\nRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (2.6.0)\nCollecting torchmetrics<0.11.0,>=0.10.2 (from unbabel-comet)\n  Downloading torchmetrics-0.10.3-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: transformers<5.0,>=4.17 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (4.49.0)\nRequirement already satisfied: PyYAML>=3.13 in /usr/local/lib/python3.10/dist-packages (from jsonargparse==3.13.1->unbabel-comet) (6.0.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (2024.12.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (24.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.20.0->unbabel-comet) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.20.0->unbabel-comet) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.20.0->unbabel-comet) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.20.0->unbabel-comet) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.20.0->unbabel-comet) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.20.0->unbabel-comet) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.1->unbabel-comet) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.1->unbabel-comet) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.1->unbabel-comet) (2025.1)\nRequirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (0.12.0)\nRequirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (3.1.1)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (2024.11.6)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (0.9.0)\nRequirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (5.3.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->unbabel-comet) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->unbabel-comet) (3.1.4)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->unbabel-comet) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->unbabel-comet) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->unbabel-comet) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->unbabel-comet) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->unbabel-comet) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->unbabel-comet) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->unbabel-comet) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->unbabel-comet) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->unbabel-comet) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->unbabel-comet) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->unbabel-comet) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->unbabel-comet) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->unbabel-comet) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->unbabel-comet) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->unbabel-comet) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.6.0->unbabel-comet) (1.3.0)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0,>=4.17->unbabel-comet) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0,>=4.17->unbabel-comet) (0.4.5)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (3.11.12)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (75.1.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.1->unbabel-comet) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->unbabel-comet) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.0.0,>=1.20.0->unbabel-comet) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.0.0,>=1.20.0->unbabel-comet) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2.0.0,>=1.20.0->unbabel-comet) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2.0.0,>=1.20.0->unbabel-comet) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.19.3->unbabel-comet) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.19.3->unbabel-comet) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.19.3->unbabel-comet) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.19.3->unbabel-comet) (2025.1.31)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.18.3)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2.0.0,>=1.20.0->unbabel-comet) (2024.2.0)\nDownloading unbabel_comet-2.2.4-py3-none-any.whl (96 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jsonargparse-3.13.1-py3-none-any.whl (101 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.4/101.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading entmax-1.3-py3-none-any.whl (13 kB)\nDownloading protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torchmetrics-0.10.3-py3-none-any.whl (529 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m529.7/529.7 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: protobuf, jsonargparse, entmax, torchmetrics, unbabel-comet\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n  Attempting uninstall: torchmetrics\n    Found existing installation: torchmetrics 1.6.1\n    Uninstalling torchmetrics-1.6.1:\n      Successfully uninstalled torchmetrics-1.6.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 4.25.6 which is incompatible.\ngoogle-cloud-bigtable 2.27.0 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\npandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\nunsloth 2025.3.13 requires protobuf<4.0.0, but you have protobuf 4.25.6 which is incompatible.\nunsloth-zoo 2025.3.11 requires protobuf<4.0.0, but you have protobuf 4.25.6 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed entmax-1.3 jsonargparse-3.13.1 protobuf-4.25.6 torchmetrics-0.10.3 unbabel-comet-2.2.4\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Loading Necessary Libraries\nfrom datasets import load_dataset, DatasetDict\nimport json\nimport os\nimport zipfile\n\n# Loading the dataset from Hugging Face Datasets library\n# We are loading the \"opus100\" dataset with the \"de-fr\" language pair (German to French).\n# We are selecting the \"test\" split, shuffling it with a seed for reproducibility, and taking the first 1000 examples.\ndataset = load_dataset(\"opus100\", \"de-fr\")[\"test\"].shuffle(seed=42).select(range(1000))\n\n# Splitting the selected dataset into training and testing sets.\n# We are using a 80/20 split, with 80% for training and 20% for testing.\n# The seed is set to 9 for reproducibility.\ntrain_test_split = dataset.train_test_split(test_size=0.2, seed=9)\ntrain_data, test_data = train_test_split[\"train\"], train_test_split[\"test\"]\n\n# Creating a directory to store the dataset splits.\nos.makedirs(\"datasets\", exist_ok=True)\n\n# Creating a dictionary to hold the training and testing data.\nsplits = {\n    \"train\": train_data,\n    \"test\": test_data\n}\n\n# Saving the training and testing data as JSON files.\n# We iterate through the splits dictionary and save each split as a JSON file.\nfor split_name, split_data in splits.items():\n    file_path = f\"datasets/{split_name}.json\"\n    # Convert the dataset to a dictionary and save it as a JSON file.\n    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(split_data.to_dict(), f, indent=4, ensure_ascii=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:11:59.009810Z","iopub.execute_input":"2025-03-14T16:11:59.010048Z","iopub.status.idle":"2025-03-14T16:12:04.171032Z","shell.execute_reply.started":"2025-03-14T16:11:59.010024Z","shell.execute_reply":"2025-03-14T16:12:04.170224Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/65.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37dac182f9654c29b75e2d35ac9c6ae0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/312k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a6ae59510cf4202852e24640b6f560e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90cc9116cdfa4e019970b924c5bcc32a"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Load the Pre-trained Model (Model A)\n# We use `FastLanguageModel` from `unsloth` to load the chosen pre-trained model.\n# We use 4-bit quantization for memory efficiency.\n\nfrom unsloth import FastLanguageModel\nimport torch\n\n# Setting the maximum sequence length for the model.\nmax_seq_length = 2048\n\n# Setting the data type (None means it will be automatically determined).\ndtype = None\n\n# Enabling 4-bit quantization for memory efficiency.\nload_in_4bit = True\n\n# Load the model and tokenizer from the specified pre-trained model.\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/Qwen2.5-1.5B-Instruct\",\n    max_seq_length=max_seq_length,\n    dtype=dtype,\n    load_in_4bit=load_in_4bit,\n)\n\n# Apply LoRA (Low-Rank Adaptation) to the pre-trained model.\n# LoRA allows for efficient fine-tuning by only training a small number of parameters.\nmodel = FastLanguageModel.get_peft_model(\n        model,\n        r=16,\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                        \"gate_proj\", \"up_proj\", \"down_proj\"],\n        lora_alpha=16,\n        lora_dropout=0,\n        bias=\"none\",\n        use_gradient_checkpointing=\"unsloth\",\n        random_state=3407,\n        use_rslora=False,\n        loftq_config=None,\n    )\n\n# Assigning model to model_a\nmodel_a = model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:12:04.171881Z","iopub.execute_input":"2025-03-14T16:12:04.172275Z","iopub.status.idle":"2025-03-14T16:12:54.307014Z","shell.execute_reply.started":"2025-03-14T16:12:04.172253Z","shell.execute_reply":"2025-03-14T16:12:54.306317Z"}},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n🦥 Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.3.13: Fast Qwen2 patching. Transformers: 4.49.0.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.53G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19d99f730e9640bb8a74b8f61aecfca0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/270 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68a8b8a1432a484c9df2289d9a732e19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.36k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"494d02ca7a6c492fb70585d00a99b0f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea378883c70a446fb45895325b177605"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d655aaec6e89492fbe32d4edfe51ff69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8db312ccc494b158542d791742cd0be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac22357fe7ba414fa2324cd5cf290993"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34aa7f3ed4ce45049ef582fecf94c34f"}},"metadata":{}},{"name":"stderr","text":"Unsloth 2025.3.13 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Define prompt formatting function\ndef format_prompt(text):\n    \"\"\"\n    Formats a German text input into a structured prompt for a translation model.\n\n    The formatted prompt follows a strict set of translation rules, ensuring:\n    - Proper nouns, technical terms, and numbers remain unchanged\n    - No explanations or extra notes are added\n    - Only the French translation is returned\n    - Original formatting and punctuation are preserved\n    - The output is strictly in French\n\n    Args:\n        text (str): The German text to be translated.\n\n    Returns:\n        str: A structured prompt including the input text for the translation model.\n    \"\"\"    \n    return f\"\"\"<|im_start|>system\nYou are a professional German-to-French translator. Follow these rules STRICTLY:\n1. Translate ALL text to French EXCEPT:\n   - Proper nouns (names, laws, organizations)\n   - Technical terms without direct equivalents\n   - Numbers/measurements\n2. Never add explanations or notes\n3. Output ONLY the French translation\n4. Maintain original formatting and punctuation\n5. Never include any non-French text\n\nExample Input: \"Die Umsetzung der Richtlinie 95/46/EG wird überprüft.\"\nExample Output: \"La mise en œuvre de la directive 95/46/CE fait l'objet d'un examen.\"\n\nNow translate:\n{text}<|im_end|>\n<|im_start|>assistant\n\"\"\"\n\ndef preprocess_function(examples):\n    \"\"\"\n    Preprocesses a batch of translation examples for training a language model.\n\n    This function prepares German-to-French translation pairs by formatting the input prompts,\n    tokenizing both source and target texts, padding/truncating them to a fixed length, and\n    masking padding tokens in the target sequence.\n\n    Args:\n        examples (dict): A batch of examples containing a \"translation\" key.\n                         Each example is a dictionary with keys:\n                         - 'de': Source text in German\n                         - 'fr': Target text in French\n\n    Returns:\n        dict: A dictionary containing tokenized inputs and labels:\n              - \"input_ids\": Tokenized German input prompts\n              - \"attention_mask\": Attention masks for input sequences\n              - \"labels\": Tokenized French target sequences with padding tokens masked as -100\n    \"\"\"\n    prompts = [format_prompt(text['de']) for text in examples[\"translation\"]]\n    targets = [text[\"fr\"] for text in examples[\"translation\"]]\n    model_inputs = tokenizer(prompts, max_length=256, truncation=True, padding=\"max_length\")\n    labels = tokenizer(targets, max_length=256, truncation=True, padding=\"max_length\")\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n\n    # Mask padding tokens\n    model_inputs[\"labels\"] = [\n        [-100 if token == tokenizer.pad_token_id else token for token in label]\n        for label in model_inputs[\"labels\"]\n    ]\n\n    return model_inputs\n\n# Process train and test dataset for evaluation\n\ntokenized_train = train_data.map(preprocess_function, batched=True, remove_columns=test_data.column_names)\n\ntokenized_test = test_data.map(preprocess_function, batched=True, remove_columns=test_data.column_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:12:54.307928Z","iopub.execute_input":"2025-03-14T16:12:54.308151Z","iopub.status.idle":"2025-03-14T16:12:56.323353Z","shell.execute_reply.started":"2025-03-14T16:12:54.308131Z","shell.execute_reply":"2025-03-14T16:12:56.322413Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"126bc825205e47b28467cf10c674641c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf487b5cb3e24541b721a69e51f06455"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"from comet import download_model, load_from_checkpoint\n\n# Download and load COMET model\n# Downloads the specified COMET model (\"Unbabel/wmt22-comet-da\") if it's not already present locally.\n# Returns the local path to the downloaded model.\ncomet_model_path = download_model(\"Unbabel/wmt22-comet-da\")\n\n# Loads the COMET model from the specified checkpoint path.\n# Returns a loaded COMET model object.\ncomet_model = load_from_checkpoint(comet_model_path)\n\ndef compute_comet(model, tokenizer, dataset):\n    \"\"\"\n    Computes COMET scores for translations generated by a given model.\n\n    Args:\n        model: The model used to generate translations.\n        tokenizer: The tokenizer associated with the model.\n        dataset: The dataset containing source texts and reference translations.\n\n    Returns:\n        A tuple containing:\n            - comet_scores: A dictionary containing COMET scores.\n            - translations: A list of generated translations.\n            - references: A list of reference translations.\n    \"\"\"\n    \n    translations = []\n    sources = []\n    references = []\n    \n    for ex in dataset:\n        \n        # Format the prompt for the model.\n        prompt = format_prompt(ex[\"translation\"][\"de\"])\n\n        # Tokenize the prompt and move it to the model's device.\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\n        # Generate translations using the model.\n        # Generate text with max_new_tokens = 150, and disable sampling.\n        # pad_token_id is set to eos_token_id to avoid padding issues.\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            do_sample=False,\n            pad_token_id=tokenizer.eos_token_id\n        )\n\n        # Decode the generated output and extract the assistant's response.\n        # skip_special_tokens=False to preserve special tokens during decoding.\n        full_output = tokenizer.decode(outputs[0], skip_special_tokens=False)\n\n        # Extract the translation from the full output.\n        # Assumes the generated text is in the format \"<|im_start|>assistant...<|im_end|>\".\n        translation = full_output.split(\"<|im_start|>assistant\")[1].split(\"<|im_end|>\")[0].strip()\n\n        # Append the translation, source, and reference to their respective lists.\n        translations.append(translation)\n        sources.append(ex[\"translation\"][\"de\"])\n        references.append(ex[\"translation\"][\"fr\"])\n\n    # Prepare inputs for the COMET model.\n    # Create a list of dictionaries, where each dictionary contains the source, machine translation, and reference.\n    comet_inputs = [{\"src\": s, \"mt\": t, \"ref\": r} for s, t, r in zip(sources, translations, references)]\n\n    # Compute COMET scores using the COMET model.\n    # Batch size is set to 8 for efficiency.\n    comet_scores = comet_model.predict(comet_inputs, batch_size=8)\n\n    # Return the COMET scores, translations, and references.\n    return comet_scores, translations, references","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:14:24.348049Z","iopub.execute_input":"2025-03-14T16:14:24.348410Z","iopub.status.idle":"2025-03-14T16:14:51.796668Z","shell.execute_reply.started":"2025-03-14T16:14:24.348379Z","shell.execute_reply":"2025-03-14T16:14:51.795594Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afc0348554124a74ad529b6208bc1dce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"LICENSE:   0%|          | 0.00/9.69k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d2b8f840cd6479984f87f774d562dcc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes:   0%|          | 0.00/1.48k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e1fba52361e4c3da1e6d8d863fc04d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/3.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca74bd837fa04f7faddc26653823bef9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"hparams.yaml:   0%|          | 0.00/567 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecbcd479401d409ca859a7debff0c256"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.ckpt:   0%|          | 0.00/2.32G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebec0de1141447db854b9bf6e1f82636"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf3061eb49e543eeba83dda3b1b44bc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"940c6c599b944e489f6616723a42d9c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28319f0c92604498bfc811be67e94aae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b583e11180148e7840eb2851084c48f"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Evaluate Model A on test set\ncomet_results, gen_translations, ref_translations = compute_comet(model_a, tokenizer, test_data)\ncomet_score_model_A = sum(comet_results['scores'])/len(comet_results['scores'])\nprint(f\"Model A Average COMET Score: {comet_score_model_A}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:14:51.797734Z","iopub.execute_input":"2025-03-14T16:14:51.798097Z","iopub.status.idle":"2025-03-14T16:20:35.216605Z","shell.execute_reply.started":"2025-03-14T16:14:51.798070Z","shell.execute_reply":"2025-03-14T16:20:35.215528Z"}},"outputs":[{"name":"stderr","text":"Predicting DataLoader 0: 100%|██████████| 25/25 [00:06<00:00,  4.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Model A Average COMET Score: 0.6674518914520741\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Fine-Tune Model A on Training Data → Model B\n\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments, DataCollatorForSeq2Seq\nfrom unsloth import is_bfloat16_supported\n\n# Determine if bf16 is supported\nuse_bf16 = is_bfloat16_supported()\n\n# Define Training Arguments\n# Configures the training process with various hyperparameters and settings.\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=100,\n    learning_rate=3e-5,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    fp16=not use_bf16,\n    bf16=use_bf16,\n    logging_strategy=\"steps\",\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    report_to=\"none\",\n    optim=\"adamw_8bit\",\n    warmup_ratio=0.1,\n    max_grad_norm=1.0\n)\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=tokenizer.pad_token_id)\n\n# Initialize Trainer\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_test,\n    data_collator=data_collator,\n    packing=True\n)\n\n# Train the Model\ntrainer.train()\n\n\n# Assign Fine-tuned Model\nmodel_b = model  # After fine-tuning, set Model B\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:20:35.218746Z","iopub.execute_input":"2025-03-14T16:20:35.219032Z","iopub.status.idle":"2025-03-14T16:28:20.982374Z","shell.execute_reply.started":"2025-03-14T16:20:35.219005Z","shell.execute_reply":"2025-03-14T16:28:20.981340Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.36k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29d5f4b397434bc68e0c66f8cfb06999"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efdc9734f45d40648861f4673f59efd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b25c323336a940c0bb78e21097c2cd03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7321e47203c743e288c1fbf783778c36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65631afdaabd4f8ea836c1a96a735625"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32f7b008cd7d436c8d92713bb652268b"}},"metadata":{}},{"name":"stdout","text":"Unsloth: Hugging Face's packing is currently buggy - we're disabling it for now!\nUnsloth: Hugging Face's packing is currently buggy - we're disabling it for now!\n","output_type":"stream"},{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 800 | Num Epochs = 3 | Total steps = 75\nO^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 8\n\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 8 x 1) = 32\n \"-____-\"     Trainable parameters = 18,464,768/5,000,000,000 (0.37% trained)\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [75/75 07:33, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>12.004900</td>\n      <td>11.132334</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Unsloth: Not an error, but Qwen2ForCausalLM does not accept `num_items_in_batch`.\nUsing gradient accumulation will be very slightly less accurate.\nRead more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Evaluate Model B on test set\ncomet_results, gen_translations, ref_translations = compute_comet(model_b, tokenizer, test_data)\ncomet_score_model_B = sum(comet_results['scores'])/len(comet_results['scores'])\nprint(f\"Model B Average COMET Score: {comet_score_model_B}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:28:20.983806Z","iopub.execute_input":"2025-03-14T16:28:20.984143Z","iopub.status.idle":"2025-03-14T16:34:17.121327Z","shell.execute_reply.started":"2025-03-14T16:28:20.984111Z","shell.execute_reply":"2025-03-14T16:34:17.120295Z"}},"outputs":[{"name":"stderr","text":"Predicting DataLoader 0: 100%|██████████| 25/25 [00:06<00:00,  3.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Model B Average COMET Score: 0.6687500277161598\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Generate Synthetic Dataset (Dataset B)\n# We use a designed prompt and the Together.ai API to generate synthetic German-French pairs.\n\nimport json\nimport requests\nimport time\nimport random\n\n# Re-load dataset for synthetic generation from the test split (for demonstration)\ndataset_syn = load_dataset(\"opus100\", \"de-fr\")[\"test\"].shuffle(seed=42).select(range(1000))\ntrain_size = int(0.8 * len(dataset_syn))\ntrain_dataset_syn = dataset_syn.select(range(train_size))\ntest_dataset_syn = dataset_syn.select(range(train_size, len(dataset_syn)))\n\n# Together.ai API details\nAPI_URL = \"https://api.together.xyz/v1/chat/completions\"\nHEADERS = {\"Authorization\": \"Bearer 9677452b60250298f9f10fd6c195d72ba07334e0947922ed5d86c9d3b2bfd159\", \"Content-Type\": \"application/json\"}\n\n# Topic categories for diverse translations\nTOPICS = [\n    \"Casual conversation\", \"Technology\", \"Healthcare\", \"Legal\", \"Business\",\n    \"Education\", \"Travel\", \"Entertainment\", \"Science\", \"Environment\", \"History\", \"Sports\"\n]\n\nPROMPT_TEMPLATE = \"\"\"\nGenerate a synthetic German-French parallel sentence pair based on the following category: {topic}.\nEnsure the sentence is natural, well-formed, and contextually relevant.\n\nExample input sentence:\nGerman: \"{german_sentence}\"\nFrench: \"{french_sentence}\"\n\nNow generate a NEW synthetic German-French sentence pair on the topic '{topic}'.\nProvide only the output in JSON format:\n{{\n    \"german\": \"...\",\n    \"french\": \"...\"\n}}\n\"\"\"\n\ndef generate_synthetic_translation(german, french):\n    \"\"\"\n    Generates a synthetic German-French translation pair using the Together.ai API.\n\n    Args:\n        german (str): The original German sentence.\n        french (str): The original French sentence.\n\n    Returns:\n        tuple: A tuple containing the synthetic German and French sentences, or (\"\", \"\") if an error occurs.\n    \"\"\"\n\n    topic = random.choice(TOPICS)\n    prompt = PROMPT_TEMPLATE.format(german_sentence=german, french_sentence=french, topic=topic)\n    payload = {\n        \"model\": \"Qwen/Qwen2.5-Coder-32B-Instruct\",\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n        \"temperature\": 0.7\n    }\n    try:\n        response = requests.post(API_URL, headers=HEADERS, json=payload)\n        response_json = response.json()\n        generated_text = response_json.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\").strip()\n        synthetic_pair = json.loads(generated_text)\n        return synthetic_pair.get(\"german\", \"\"), synthetic_pair.get(\"french\", \"\")\n    except Exception as e:\n        print(\"Error:\", e)\n        return \"\", \"\"\n\n# Generate twice the size of train data synthetic samples (2 per original sentence)\nsynthetic_data = []\nfor i, sample in enumerate(train_dataset_syn):\n    german_text = sample[\"translation\"][\"de\"]\n    french_text = sample[\"translation\"][\"fr\"]\n    for _ in range(2):\n        synthetic_german, synthetic_french = generate_synthetic_translation(german_text, french_text)\n        if synthetic_german and synthetic_french:\n            synthetic_data.append({\"translation\": {\"de\": synthetic_german, \"fr\": synthetic_french}})\n    # print(f\"Processed {i+1}/{len(train_dataset_syn)} samples... Synthetic data size: {len(synthetic_data)}\") # For Logging\n    if len(synthetic_data) >= 1600:\n        break\n    time.sleep(1)\n\n# Save synthetic dataset as JSON (Dataset B)\nwith open(\"datasets/synthetic.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(synthetic_data, f, indent=4, ensure_ascii=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T12:54:46.475681Z","iopub.status.idle":"2025-03-13T12:54:46.475967Z","shell.execute_reply":"2025-03-13T12:54:46.475852Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict\nimport json\n\n# Load the JSON file\nwith open(\"/kaggle/input/synthetic-dataset-1/synthetic_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\n# Ensure data is in the correct format\nhf_data = [{\"translation\": {\"de\": pair[\"de\"], \"fr\": pair[\"fr\"]}} for pair in data]\n\n# Create a Hugging Face Dataset\nhf_dataset = Dataset.from_list(hf_data)\n\n# Print an example\nprint(hf_dataset[0])\n\n# Optionally, save it to disk\nhf_dataset.save_to_disk(\"de_fr_dataset\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:34:17.122656Z","iopub.execute_input":"2025-03-14T16:34:17.123001Z","iopub.status.idle":"2025-03-14T16:34:17.230989Z","shell.execute_reply.started":"2025-03-14T16:34:17.122967Z","shell.execute_reply":"2025-03-14T16:34:17.230083Z"}},"outputs":[{"name":"stdout","text":"{'translation': {'de': 'Das neue Spiel im Kino wird von einem berühmten Schauspieler vorgespielt.', 'fr': 'Le nouveau film au cinéma est interprété par un célèbre acteur.'}}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/1600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46144fe4f7e5499e84dabbf6c1fe25ff"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# Load the dataset from disk\nfrom datasets import load_from_disk\n\nsynthetic_dataset = load_from_disk(\"de_fr_dataset\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:34:17.231976Z","iopub.execute_input":"2025-03-14T16:34:17.232278Z","iopub.status.idle":"2025-03-14T16:34:17.239377Z","shell.execute_reply.started":"2025-03-14T16:34:17.232247Z","shell.execute_reply":"2025-03-14T16:34:17.238642Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Process synthetic dataset for evaluation\n\ntokenized_synthetic_data = synthetic_dataset.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=train_data.column_names\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:34:17.241955Z","iopub.execute_input":"2025-03-14T16:34:17.242158Z","iopub.status.idle":"2025-03-14T16:34:20.047565Z","shell.execute_reply.started":"2025-03-14T16:34:17.242140Z","shell.execute_reply":"2025-03-14T16:34:20.046869Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2179ab050db04ace846ce35771826ee4"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# Reloading my model for finetuning\nfrom unsloth import FastLanguageModel\nimport torch\n\nmax_seq_length = 2048  # Maximum sequence length\ndtype = None           # Auto detection\nload_in_4bit = True    # Use 4bit quantization\n\n# Load the model and tokenizer\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/Qwen2.5-1.5B-Instruct\",\n    max_seq_length=max_seq_length,\n    dtype=dtype,\n    load_in_4bit=load_in_4bit,\n)\n\nmodel = FastLanguageModel.get_peft_model(\n        model,\n        r=16,  # LoRA rank\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                        \"gate_proj\", \"up_proj\", \"down_proj\"],\n        lora_alpha=16,\n        lora_dropout=0,\n        bias=\"none\",\n        use_gradient_checkpointing=\"unsloth\",\n        random_state=3407,\n        use_rslora=False,\n        loftq_config=None,\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:34:20.048973Z","iopub.execute_input":"2025-03-14T16:34:20.049259Z","iopub.status.idle":"2025-03-14T16:34:32.182455Z","shell.execute_reply.started":"2025-03-14T16:34:20.049237Z","shell.execute_reply":"2025-03-14T16:34:32.181719Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.3.13: Fast Qwen2 patching. Transformers: 4.49.0.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Fine-Tune Model A on Synthetic Data → Model C\n\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments, DataCollatorForSeq2Seq\nfrom unsloth import is_bfloat16_supported\n\n# Determine if bf16 is supported\nuse_bf16 = is_bfloat16_supported()\n\n# Define Training Arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=100,\n    learning_rate=3e-5,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    fp16=not use_bf16,\n    bf16=use_bf16,\n    logging_strategy=\"steps\",\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    report_to=\"none\",\n    optim=\"adamw_8bit\",\n    warmup_ratio=0.1,\n    max_grad_norm=1.0\n)\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=tokenizer.pad_token_id)\n\n# Initialize Trainer\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_synthetic_data,\n    eval_dataset=tokenized_test,\n    data_collator=data_collator,\n    packing=True\n)\ntrainer.train()\n\nmodel_c = model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:34:32.183259Z","iopub.execute_input":"2025-03-14T16:34:32.183520Z","iopub.status.idle":"2025-03-14T16:50:05.775049Z","shell.execute_reply.started":"2025-03-14T16:34:32.183498Z","shell.execute_reply":"2025-03-14T16:50:05.773872Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Hugging Face's packing is currently buggy - we're disabling it for now!\nUnsloth: Hugging Face's packing is currently buggy - we're disabling it for now!\n","output_type":"stream"},{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 1,600 | Num Epochs = 3 | Total steps = 150\nO^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 8\n\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 8 x 1) = 32\n \"-____-\"     Trainable parameters = 18,464,768/5,000,000,000 (0.37% trained)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [150/150 15:24, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>10.514700</td>\n      <td>10.233426</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>7.545900</td>\n      <td>7.975505</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>7.203800</td>\n      <td>7.895988</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# Evaluate Model C on test set\ncomet_results, gen_translations, ref_translations = compute_comet(model_c, tokenizer, test_data)\ncomet_score_model_C = sum(comet_results['scores'])/len(comet_results['scores'])\nprint(f\"Model C Average COMET Score: {comet_score_model_C}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:50:05.776108Z","iopub.execute_input":"2025-03-14T16:50:05.776535Z","iopub.status.idle":"2025-03-14T16:56:15.489028Z","shell.execute_reply.started":"2025-03-14T16:50:05.776474Z","shell.execute_reply":"2025-03-14T16:56:15.488087Z"}},"outputs":[{"name":"stderr","text":"Predicting DataLoader 0: 100%|██████████| 25/25 [00:06<00:00,  3.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Model C Average COMET Score: 0.6519817470759154\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Reloading my model for finetuning\nfrom unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Qwen2.5-1.5B-Instruct\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:56:15.490135Z","iopub.execute_input":"2025-03-14T16:56:15.490388Z","iopub.status.idle":"2025-03-14T16:56:28.936966Z","shell.execute_reply.started":"2025-03-14T16:56:15.490362Z","shell.execute_reply":"2025-03-14T16:56:28.936180Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.3.13: Fast Qwen2 patching. Transformers: 4.49.0.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Combine Training Data and Synthetic Data\nfrom datasets import concatenate_datasets\ncombined_dataset = concatenate_datasets([tokenized_train, tokenized_synthetic_data])\n\n# Save combined dataset as JSON (Dataset C)\ncombined_dict = combined_dataset.to_dict()\nwith open(\"datasets/combined.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(combined_dict, f, indent=4, ensure_ascii=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:56:28.937869Z","iopub.execute_input":"2025-03-14T16:56:28.938105Z","iopub.status.idle":"2025-03-14T16:56:30.742066Z","shell.execute_reply.started":"2025-03-14T16:56:28.938086Z","shell.execute_reply":"2025-03-14T16:56:30.741024Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Fine-Tune Model A on Combined Data → Model D\n\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments, DataCollatorForSeq2Seq\nfrom unsloth import is_bfloat16_supported\n\nuse_bf16 = is_bfloat16_supported()\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"steps\",\n    eval_steps=500,  # Less frequent evaluation to avoid noise\n    save_steps=500,  # Save less frequently to prevent overfitting\n    learning_rate=1e-5,  # Lower LR to avoid catastrophic forgetting\n    per_device_train_batch_size=8,  # Larger batch size for stability\n    per_device_eval_batch_size=8,\n    gradient_accumulation_steps=2,  # Lower accumulation for better adaptation\n    num_train_epochs=2,  # Reduce training duration to prevent overfitting\n    weight_decay=0.02,  # Moderate regularization\n    fp16=not use_bf16,\n    bf16=use_bf16,\n    logging_strategy=\"steps\",\n    logging_steps=50,  # Log less frequently\n    save_strategy=\"epoch\",\n    report_to=\"none\",\n    optim=\"adamw_torch_fused\",  # More stable optimizer for LLMs\n    warmup_ratio=0.3,  # Longer warmup for a gradual learning curve\n    max_grad_norm=0.3,  # Reduce gradient clipping further\n)\n\n# Use Seq2Seq Data Collator for Translation\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=tokenizer.pad_token_id)\n\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=combined_dataset,\n    eval_dataset=tokenized_test,\n    data_collator=data_collator,\n    packing=False,\n)\n\ntrainer.train()\nmodel_d = model\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:56:30.743171Z","iopub.execute_input":"2025-03-14T16:56:30.743414Z","iopub.status.idle":"2025-03-14T17:08:49.144600Z","shell.execute_reply.started":"2025-03-14T16:56:30.743394Z","shell.execute_reply":"2025-03-14T17:08:49.143577Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 2,400 | Num Epochs = 2 | Total steps = 150\nO^O/ \\_/ \\    Batch size per device = 16 | Gradient accumulation steps = 2\n\\        /    Data Parallel GPUs = 1 | Total batch size (16 x 2 x 1) = 32\n \"-____-\"     Trainable parameters = 18,464,768/5,000,000,000 (0.37% trained)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [150/150 12:04, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"# Evaluate Model D on test set\ncomet_results, gen_translations, ref_translations = compute_comet(model_d, tokenizer, test_data)\ncomet_score_model_D = sum(comet_results['scores'])/len(comet_results['scores'])\nprint(f\"Model D Average COMET Score: {comet_score_model_D}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T17:08:49.145777Z","iopub.execute_input":"2025-03-14T17:08:49.146153Z","iopub.status.idle":"2025-03-14T17:14:42.077014Z","shell.execute_reply.started":"2025-03-14T17:08:49.146109Z","shell.execute_reply":"2025-03-14T17:14:42.076068Z"}},"outputs":[{"name":"stderr","text":"Predicting DataLoader 0: 100%|██████████| 25/25 [00:06<00:00,  3.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Model D Average COMET Score: 0.6677549533545971\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# %% [markdown]\n# ### Step 12: Plot Performance of All Models\n# \n# We plot the COMET scores for Models A, B, C, and D.\n\n# %% [code]\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# For demonstration, we assume we stored the COMET scores after each evaluation.\n# Replace these placeholders with your actual computed mean scores.\nmodels = [\"Model A\", \"Model B\", \"Model C\", \"Model D\"]\ncomet_scores = [\n    comet_score_model_A,  # Placeholder for Model A\n    comet_score_model_B,  # Placeholder for Model B\n    comet_score_model_C,  # Placeholder for Model C\n    comet_score_model_D   # Placeholder for Model D\n]\n\nplt.figure(figsize=(8, 5))\nbars = plt.bar(models, comet_scores, color=['blue', 'green', 'red', 'purple'])\nplt.xlabel(\"Models\")\nplt.ylabel(\"Average COMET Score\")\nplt.title(\"COMET Scores for Different Fine-Tuned Models\")\nplt.ylim(0.5, 1.0)\n\n# Annotate bar values\nfor bar, score in zip(bars, comet_scores):\n    yval = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.01, f\"{score:.2f}\", ha='center', fontsize=12)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T17:14:42.078091Z","iopub.execute_input":"2025-03-14T17:14:42.078424Z","iopub.status.idle":"2025-03-14T17:14:42.398200Z","shell.execute_reply.started":"2025-03-14T17:14:42.078396Z","shell.execute_reply":"2025-03-14T17:14:42.397290Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x500 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"# %% [markdown]\n# ### Save the Fine-Tuned Models and Tokenizer\n# \n# We now save all models for later use in the Streamlit interface.\n\n# %% [code]\n# Make sure the model variables exist.\ntry:\n    print('model is saving')\n    model_a.save_pretrained(\"./saved_models/model_a\")\n    model_b.save_pretrained(\"./saved_models/model_b\")\n    model_c.save_pretrained(\"./saved_models/model_c\")\n    model_d.save_pretrained(\"./saved_models/model_d\")\n    tokenizer.save_pretrained(\"./saved_models/tokenizer\")\n    print('model saved')\nexcept Exception as e:\n    print(\"Error saving models:\", e)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T17:46:37.543713Z","iopub.execute_input":"2025-03-14T17:46:37.544077Z","iopub.status.idle":"2025-03-14T17:46:39.665177Z","shell.execute_reply.started":"2025-03-14T17:46:37.544046Z","shell.execute_reply":"2025-03-14T17:46:39.664114Z"}},"outputs":[{"name":"stdout","text":"model is saving\nmodel saved\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# %% [markdown]\n# ### Create ZIP Archive for Dataset Splits\n# \n# We create a ZIP file that contains the following JSON files:\n# - Training [Dataset A: Train] (`datasets/train.json`)\n# - Testing [Dataset A: Test] (`datasets/test.json`)\n# - Synthesized [Dataset B] (`datasets/synthetic.json`)\n# - Combined [Dataset C] (`datasets/combined.json`)\n\n# %% [code]\nwith zipfile.ZipFile(\"dataset_splits.zip\", \"w\") as zipf:\n    for file_name in [\"datasets/train.json\", \"datasets/test.json\", \"datasets/synthetic.json\", \"datasets/combined.json\"]:\n        zipf.write(file_name)\nprint(\"Dataset splits saved in dataset_splits.zip\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:12:56.353800Z","iopub.status.idle":"2025-03-14T16:12:56.354099Z","shell.execute_reply":"2025-03-14T16:12:56.353992Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install gradio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T17:16:53.193130Z","iopub.execute_input":"2025-03-14T17:16:53.193502Z","iopub.status.idle":"2025-03-14T17:17:04.151466Z","shell.execute_reply.started":"2025-03-14T17:16:53.193455Z","shell.execute_reply":"2025-03-14T17:17:04.150557Z"}},"outputs":[{"name":"stdout","text":"Collecting gradio\n  Downloading gradio-5.21.0-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\nCollecting fastapi<1.0,>=0.115.2 (from gradio)\n  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\nCollecting ffmpy (from gradio)\n  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\nCollecting gradio-client==1.7.2 (from gradio)\n  Downloading gradio_client-1.7.2-py3-none-any.whl.metadata (7.1 kB)\nCollecting groovy~=0.1 (from gradio)\n  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\nRequirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.29.0)\nRequirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\nCollecting markupsafe~=2.0 (from gradio)\n  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\nRequirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\nRequirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.12)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\nRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.3)\nRequirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\nRequirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.11.0a2)\nRequirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\nCollecting python-multipart>=0.0.18 (from gradio)\n  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\nCollecting ruff>=0.9.3 (from gradio)\n  Downloading ruff-0.11.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\nCollecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\nCollecting semantic-version~=2.0 (from gradio)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\nCollecting starlette<1.0,>=0.40.0 (from gradio)\n  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\nCollecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\nRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\nCollecting uvicorn>=0.14.0 (from gradio)\n  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.7.2->gradio) (2024.12.0)\nRequirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.7.2->gradio) (14.1)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.17.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.29.0)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.0->gradio) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\nDownloading gradio-5.21.0-py3-none-any.whl (46.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gradio_client-1.7.2-py3-none-any.whl (322 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\nDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\nDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\nDownloading ruff-0.11.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\nDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nDownloading starlette-0.46.1-py3-none-any.whl (71 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\nDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\nInstalling collected packages: uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, groovy, ffmpy, starlette, safehttpx, gradio-client, fastapi, gradio\n  Attempting uninstall: markupsafe\n    Found existing installation: MarkupSafe 3.0.2\n    Uninstalling MarkupSafe-3.0.2:\n      Successfully uninstalled MarkupSafe-3.0.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\nunsloth 2025.3.13 requires protobuf<4.0.0, but you have protobuf 4.25.6 which is incompatible.\nunsloth-zoo 2025.3.11 requires protobuf<4.0.0, but you have protobuf 4.25.6 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fastapi-0.115.11 ffmpy-0.5.0 gradio-5.21.0 gradio-client-1.7.2 groovy-0.1.2 markupsafe-2.1.5 python-multipart-0.0.20 ruff-0.11.0 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tomlkit-0.13.2 uvicorn-0.34.0\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import gradio as gr\nfrom unsloth import FastLanguageModel\nfrom transformers import AutoTokenizer\nimport torch\n\ndef format_prompt(text):\n    return f\"\"\"<|im_start|>system\nYou are a professional German-to-French translator. Follow these rules STRICTLY:\n1. Translate ALL text to French EXCEPT:\n   - Proper nouns (names, laws, organizations)\n   - Technical terms without direct equivalents\n   - Numbers/measurements\n2. Never add explanations or notes\n3. Output ONLY the French translation\n4. Maintain original formatting and punctuation\n5. Never include any non-French text\n\nNow translate:\n{text}<|im_end|>\n<|im_start|>assistant\n\"\"\"\n\n# Load model with caching\ndef load_model():\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name = \"/kaggle/working/saved_models/model_d\",\n        max_seq_length = 2048,\n        dtype = None,\n        load_in_4bit = True,\n    )\n    return model, tokenizer\n\nmodel, tokenizer = load_model()\n\ndef translate(text):\n    if not text.strip():\n        return \"Please enter some German text to translate\"\n        \n    prompt = format_prompt(text)\n    inputs = tokenizer(prompt, return_tensors=\"pt\", \n                     truncation=True, max_length=256).to(model.device)\n    \n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=256,\n        do_sample=False,\n        pad_token_id=tokenizer.eos_token_id\n    )\n    \n    full_output = tokenizer.decode(outputs[0], skip_special_tokens=False)\n    translation = full_output.split(\"<|im_start|>assistant\")[1]\n    return translation.split(\"<|im_end|>\")[0].strip()\n\n# Create Gradio interface\ninterface = gr.Interface(\n    fn=translate,\n    inputs=gr.Textbox(label=\"German Input\", lines=5),\n    outputs=gr.Textbox(label=\"French Translation\", lines=5),\n    title=\"🇩🇪→🇫🇷 Professional Translator\",\n    description=\"German to French translation using fine-tuned Qwen2.5-1.5B-Instruct\",\n    examples=[\n        [\"Die Umsetzung der Richtlinie 95/46/EG wird überprüft.\"],\n        [\"Der technische Fortschritt erfordert neue regulatorische Rahmenbedingungen.\"]\n    ]\n)\n\n# Run inline in notebook\ninterface.launch(debug=True, inline=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T18:15:25.794632Z","iopub.execute_input":"2025-03-14T18:15:25.794936Z","iopub.status.idle":"2025-03-14T18:18:02.458713Z","shell.execute_reply.started":"2025-03-14T18:15:25.794914Z","shell.execute_reply":"2025-03-14T18:18:02.457843Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.3.13: Fast Qwen2 patching. Transformers: 4.49.0.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n* Running on local URL:  http://127.0.0.1:7860\nKaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://662310ba15f5c7bd1b.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://662310ba15f5c7bd1b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stdout","text":"Keyboard interruption in main thread... closing server.\nKilling tunnel 127.0.0.1:7860 <> https://662310ba15f5c7bd1b.gradio.live\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"# Was bedeutet das? \n# Es tut mir leid\n# Bringen Sie mir bitte eine Auswahl von leckeren Sachen","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}